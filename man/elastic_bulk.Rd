% Generated by roxygen2 (4.1.0): do not edit by hand
% Please edit documentation in R/elastic_functions.R
\name{elastic_bulk}
\alias{elastic_bulk}
\alias{elastic_bulk_file}
\alias{elastic_bulk_index_batch}
\title{Facilitate interaction with Elasticsearch's bulk API for CRUD operations.}
\usage{
elastic_bulk(elastic_conn, action, data, metadata = NULL)

elastic_bulk_file(action, data = NULL, metadata = NULL, file_name = NULL)

elastic_bulk_index_batch(elastic_conn, data, num_pieces = 1)
}
\arguments{
\item{elastic_conn}{connection details for the Elasticsearch server we want to access (and optionally the index and type too).}

\item{action}{one of 'index', 'update' or 'delete'.}

\item{data}{data.frame object with multiple columns representing document field values.}

\item{metadata}{data.frame object containing document metadata (index, type, id). If omitted (or NULL), then documents will automatically be indexed (i.e. created).}

\item{file_name}{the name and path (relative to the working directory), of the text file that will be produced. If NULL then a temporary file will be produced.}

\item{num_pieces}{the number of pieces to break the input data into.}
}
\value{
\code{elastic_bulk}: TRUE or FALSE depending on the success of the http request.

\code{elastic_bulk_file}: a string with the relative path and name of the text file containing the data ready to be uploaded to the Elasticsearch bulk API.

\code{elastic_bulk_batched}: TRUE or FALSE depending on the success of all http requests made to the bulk API.
}
\description{
Elasticsearch's bulk API can be awkward to use directly due to it's peculiar format. However, it remains the only way to interact with Elasticsearch
documents at scale. These functions help by allowing documents, document fields, and document metadata (e.g. index = 'my_index', type = 'my_type', id = '001'),
to be described using data frames, upon which a single bulk action can be performed ('index', 'update', or 'delete').
}
\details{
The heavy-lifting is performed by \code{elastic_bulk_file}, which generates the required JSON-based representation of the data in a text file.
All \code{elastic_bulk} does is to upload this text file to to Elasticsearch's bulk API endpoint. \code{elastic_bulk_index_batched} splits data to be indexed into
pieces first, and then applies \code{elastic_bulk} individually to each piece - the idea being that for large data each bulk upload operation can be kept to
under 15mb, which is the largest size recommended by Elasticsearch for use with the bulk API.
}
\section{Functions}{
\itemize{
\item \code{elastic_bulk_file}: 

\item \code{elastic_bulk_index_batch}: 
}}

